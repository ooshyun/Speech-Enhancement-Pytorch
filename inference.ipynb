{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import omegaconf\n",
    "from mllib.src.train import main\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.cuda\n",
    "from mllib.src.evaluate import evaluate\n",
    "from mllib.src.utils import prepare_device, load_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = \"./result/wav-unet/20230201-104328/config.yaml\"\n",
    "# config = \"./result/dcunet/20230201-104116/config.yaml\"\n",
    "\n",
    "# config = \"./result/mel-rnn/20230202-145405/config.yaml\"\n",
    "# config = './result/mel-rnn/20230203-121042/config.yaml'\n",
    "\n",
    "# config = \"./result/dnn/20230202-142249/config.yaml\"\n",
    "# config = \"./result/dnn/20230202-163959/config.yaml\"\n",
    "# config = \"./result/dnn/20230202-170504/config.yaml\"\n",
    "# config = \"./result/dnn/20230202-171624/config.yaml\"\n",
    "# config = \"./result/dnn/20230202-185453/config.yaml\"\n",
    "# config = \"./result/dnn/20230203-115011/config.yaml\"\n",
    "\n",
    "# config= \"./result/unet/20230203-183804/config.yaml\"\n",
    "\n",
    "config= \"./result/conv-tasnet/20230203-183838/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = main(path_config=config, return_solver=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = solver.test_dataloader\n",
    "model = solver.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = load_yaml(config)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = prepare_device(n_gpu, cudnn_deterministic=args.solver.cudnn_deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.default.dset.name = \"Clarity\"\n",
    "# args.default.dset.name = \"VoiceBankDEMAND\"\n",
    "\n",
    "if args.default.dset.name == \"VoiceBankDEMAND\":\n",
    "    log_voicebank = \"./mllib/data/VoiceBankDEMAND/DS_10283_2791/logfiles\"\n",
    "    text_files = glob.glob(\n",
    "        os.path.join(log_voicebank, \"*trainset*\")\n",
    "    )\n",
    "    print(text_files)\n",
    "    metadata = []\n",
    "    for text_file in text_files:\n",
    "        with open(text_file, \"r\") as tmp:\n",
    "            text = tmp.read().split(\"\\n\")\n",
    "            for i, t in enumerate(text):\n",
    "                text[i] = t.split(\" \")\n",
    "        \n",
    "            metadata.append(text)\n",
    "    print(metadata[0][0])\n",
    "\n",
    "elif args.default.dset.name == \"Clarity\":\n",
    "    log_clarity = \"./data/metadata/scenes.train.snr.json\"\n",
    "    metadata = omegaconf.OmegaConf.load(log_clarity)\n",
    "    print(list(metadata.values())[0], list(metadata.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllib.src.distrib import get_train_wav_dataset\n",
    "\n",
    "SNR = '0' # '0', '5', '10', '15' # SNR = P_{Signal} / P_{Noise}\n",
    "\n",
    "test_dataset = None\n",
    "\n",
    "if args.default.dset.name == \"VoiceBankDEMAND\":\n",
    "    args.dset.wav = './mllib/data/VoiceBankDEMAND/DS_10283_2791'\n",
    "    train_dataset, validation_dataset, test_dataset = get_train_wav_dataset(config=args.dset, name=\"VoiceBankDEMAND\")\n",
    "\n",
    "elif args.default.dset.name == \"Clarity\":\n",
    "    # TODO: Always set the root for dataset files\n",
    "    args.dset.wav = \"SET THE PATH OF WAVFILE!!!\"\n",
    "    args.default.dset.config = './recipes/icassp_2023/MLbaseline/config_train.yaml'\n",
    "    train_dataset, validation_dataset, test_dataset = get_train_wav_dataset(config=args.dset, name=\"Clarity\")\n",
    "\n",
    "dataset = test_dataset\n",
    "\n",
    "flag_find = False\n",
    "data_test = None\n",
    "\n",
    "if args.default.dset.name == \"VoiceBankDEMAND\":\n",
    "    for data in tqdm.tqdm(dataset, ncols=120):\n",
    "        mixture, clean, origial_length, name = data\n",
    "        for imetadata in range(len(metadata)):\n",
    "            for ifile in range(len(metadata[imetadata])):\n",
    "                if metadata[imetadata][ifile][0] == name:\n",
    "                    if metadata[imetadata][ifile][-1] == SNR:\n",
    "                        flag_find = True\n",
    "                    else:\n",
    "                        break\n",
    "        if flag_find:\n",
    "            data_test = data\n",
    "            break\n",
    "\n",
    "elif args.default.dset.name == \"Clarity\":\n",
    "    # data_test = dataset[0]\n",
    "    # mixture, clean, origial_length, name = data_test\n",
    "    # scene_name = name.split(\"_\")[0]\n",
    "    # SNR = metadata[scene_name]\n",
    "    # print(\"Clarity dataset SNR: \", SNR)\n",
    "\n",
    "    SNR = 5\n",
    "    for data in tqdm.tqdm(dataset, ncols=120):\n",
    "        mixture, clean, interferer, origial_length, name = data\n",
    "        scene_name = name.split(\"_\")[0]\n",
    "        if metadata[scene_name] > SNR:\n",
    "            data_test = data\n",
    "            SNR = metadata[scene_name]\n",
    "            break\n",
    "    \n",
    "    print(\"Clarity dataset SNR: \", SNR)\n",
    "\n",
    "# data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture, clean, interferer, origial_length, name = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchannel, nsample = mixture.shape\n",
    "if args.model.name in (\"demucs\", \"conv-tasnet\") and nchannel == 1:\n",
    "    try:\n",
    "        mixture = torch.cat(tensors=[mixture[None], mixture[None]], dim=1)\n",
    "        clean = torch.cat(tensors=[clean[None], clean[None]], dim=1)\n",
    "    except AttributeError:\n",
    "        # For torch 1.7.1, AttributeError: module 'torch' has no attribute 'concat'\n",
    "        mixture = torch.cat(tensors=[mixture[None], mixture[None]], dim=1)\n",
    "        clean = torch.cat(tensors=[clean[None], clean[None]], dim=1)\n",
    "\n",
    "if args.model.name not in (\"demucs\", \"conv-tasnet\"):\n",
    "    mixture = torch.reshape(mixture, shape=(nchannel, 1, nsample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture.shape, args.model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced = evaluate(mixture=mixture[None], model=model, device=device, config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert enhanced.shape[-1] == mixture.shape[-1] == clean.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced = enhanced.detach().cpu()\n",
    "if args.default.dset.name == \"Clarity\" and args.model.name in (\"demucs\", \"conv-tasnet\"):\n",
    "    enhanced_0 = enhanced[:, 0, ...]\n",
    "    enhanced_1 = enhanced[:, 1, ...]\n",
    "else:\n",
    "    enhanced_0 = enhanced\n",
    "    enhanced_1 = enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_0.shape, enhanced_1.shape, mixture.shape, clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_0_np = enhanced_0[:]\n",
    "enhanced_1_np = enhanced_1[:]\n",
    "mixture_np = mixture[:]\n",
    "clean_np = clean[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_0_np = enhanced_0.flatten().numpy()\n",
    "enhanced_1_np = enhanced_1.flatten().numpy()\n",
    "mixture_np = mixture.flatten().numpy()\n",
    "clean_np = clean.flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_0_np.shape, enhanced_1_np.shape, mixture_np.shape, clean_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows=4)\n",
    "\n",
    "ax0.plot(mixture_np)\n",
    "ax1.plot(clean_np)\n",
    "ax2.plot(enhanced_0_np)\n",
    "ax3.plot(enhanced_1_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows=4, sharey=True)\n",
    "\n",
    "def show_stft(y, _fig, _ax):\n",
    "    D = librosa.stft(y)  # STFT of y\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    img = librosa.display.specshow(S_db, ax=_ax)\n",
    "    _fig.colorbar(img, ax=_ax)\n",
    "\n",
    "show_stft(mixture_np, fig, ax0)\n",
    "show_stft(clean_np, fig, ax1)\n",
    "show_stft(enhanced_0_np, fig, ax2)\n",
    "show_stft(enhanced_1_np, fig, ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(mixture_np, rate=args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(clean_np, rate=args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(enhanced_0_np, rate=args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(enhanced_1_np, rate=args.dset.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllib.src.metric import SI_SDR, STOI, WB_PESQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [SI_SDR, STOI, WB_PESQ]\n",
    "\n",
    "clean_score = np.expand_dims(np.expand_dims(clean_np, 0), 0)\n",
    "mixture_score = np.expand_dims(np.expand_dims(mixture_np, 0), 0)\n",
    "enhanced_score = np.expand_dims(np.expand_dims(enhanced_0_np, 0), 0)\n",
    "\n",
    "print(clean_score.shape, mixture_score.shape, enhanced_score.shape)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(\"Mixture\", metric, metric(clean_score, mixture_score, sr=args.dset.sample_rate))\n",
    "    print(\"Enhanced\", metric, metric(clean_score, enhanced_score, sr=args.dset.sample_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_171_daniel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c9cb1bcb746fd43c70a7667c29c966ffe14df3c625afb960b30bce79fd091af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
